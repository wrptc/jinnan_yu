{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('jinnan_round1_train_20181227.csv', encoding = 'gb18030')\n",
    "test  = pd.read_csv('jinnan_round1_testA_20181227.csv', encoding = 'gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>样本id</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "      <th>A17</th>\n",
       "      <th>A18</th>\n",
       "      <th>A19</th>\n",
       "      <th>A20</th>\n",
       "      <th>A21</th>\n",
       "      <th>A22</th>\n",
       "      <th>A23</th>\n",
       "      <th>A24</th>\n",
       "      <th>A25</th>\n",
       "      <th>A26</th>\n",
       "      <th>A27</th>\n",
       "      <th>A28</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "      <th>B6</th>\n",
       "      <th>B7</th>\n",
       "      <th>B8</th>\n",
       "      <th>B9</th>\n",
       "      <th>B10</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>B13</th>\n",
       "      <th>B14</th>\n",
       "      <th>收率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_1528</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405.0</td>\n",
       "      <td>700</td>\n",
       "      <td>13:30:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>100</td>\n",
       "      <td>16:30:00</td>\n",
       "      <td>102</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>103.0</td>\n",
       "      <td>18:30:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>300</td>\n",
       "      <td>21:00-21:30</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>75</td>\n",
       "      <td>22:30:00</td>\n",
       "      <td>70</td>\n",
       "      <td>6:30-7:00</td>\n",
       "      <td>350.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7:00-8:00</td>\n",
       "      <td>8:00:00</td>\n",
       "      <td>65</td>\n",
       "      <td>11:30:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>11:30-13:00</td>\n",
       "      <td>14:00-15:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>400</td>\n",
       "      <td>0.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1698</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405.0</td>\n",
       "      <td>700</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>101</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>103</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>19:00-20:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>80</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>73</td>\n",
       "      <td>21:00-22:00</td>\n",
       "      <td>320.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>22:00-23:00</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>80</td>\n",
       "      <td>6:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6:00-7:30</td>\n",
       "      <td>7:30-9:00</td>\n",
       "      <td>9:00-10:00</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>400</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_639</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405.0</td>\n",
       "      <td>700</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>102</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>103</td>\n",
       "      <td>0.2</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>19:00-19:30</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>73</td>\n",
       "      <td>21:00-22:00</td>\n",
       "      <td>320.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>22:00-23:00</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>80</td>\n",
       "      <td>1:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1:00-2:30</td>\n",
       "      <td>2:30-4:00</td>\n",
       "      <td>4:00-5:00</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>400</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_483</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405.0</td>\n",
       "      <td>700</td>\n",
       "      <td>1:30:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:00:00</td>\n",
       "      <td>100</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>102</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5:00:00</td>\n",
       "      <td>103.0</td>\n",
       "      <td>6:00:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>6:30-7:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7:30:00</td>\n",
       "      <td>70</td>\n",
       "      <td>8:00:00</td>\n",
       "      <td>78</td>\n",
       "      <td>13:30-14:30</td>\n",
       "      <td>290.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>14:30-15:30</td>\n",
       "      <td>15:30:00</td>\n",
       "      <td>65</td>\n",
       "      <td>18:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>19:00-20:30</td>\n",
       "      <td>21:30-23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>400</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_617</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>405.0</td>\n",
       "      <td>700</td>\n",
       "      <td>22:00:00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>101</td>\n",
       "      <td>1:00:00</td>\n",
       "      <td>103</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2:00:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>3:00:00</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>3:00-4:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>80</td>\n",
       "      <td>5:00:00</td>\n",
       "      <td>73</td>\n",
       "      <td>5:00-6:00</td>\n",
       "      <td>320.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6:00-7:00</td>\n",
       "      <td>7:00:00</td>\n",
       "      <td>80</td>\n",
       "      <td>9:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>9:00-10:30</td>\n",
       "      <td>10:30-12:00</td>\n",
       "      <td>12:00-13:00</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>420</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          样本id   A1  A2     A3   A4        A5    A6   A7  A8        A9  A10  \\\n",
       "0  sample_1528  300 NaN  405.0  700  13:30:00  38.0  NaN NaN  15:30:00  100   \n",
       "1  sample_1698  300 NaN  405.0  700  14:00:00  29.0  NaN NaN  16:00:00  101   \n",
       "2   sample_639  300 NaN  405.0  700  14:00:00  29.0  NaN NaN  16:00:00  102   \n",
       "3   sample_483  300 NaN  405.0  700   1:30:00  38.0  NaN NaN   3:00:00  100   \n",
       "4   sample_617  300 NaN  405.0  700  22:00:00  29.0  NaN NaN   0:00:00  101   \n",
       "\n",
       "        A11  A12  A13       A14    A15       A16    A17  A18  A19  \\\n",
       "0  16:30:00  102  0.2  17:30:00  103.0  18:30:00  104.0  0.2  300   \n",
       "1  17:00:00  103  0.2  18:00:00  104.0  19:00:00  105.0  0.2  200   \n",
       "2  17:00:00  103  0.2  18:00:00  104.0  19:00:00  105.0  0.2  200   \n",
       "3   4:00:00  102  0.2   5:00:00  103.0   6:00:00  104.0  0.2  200   \n",
       "4   1:00:00  103  0.2   2:00:00  104.0   3:00:00  105.0  0.2  200   \n",
       "\n",
       "           A20   A21   A22  A23       A24 A25       A26  A27          A28  \\\n",
       "0  21:00-21:30  50.0   9.0  5.0  22:00:00  75  22:30:00   70    6:30-7:00   \n",
       "1  19:00-20:00  50.0   9.0  5.0  20:00:00  80  21:00:00   73  21:00-22:00   \n",
       "2  19:00-19:30  50.0   9.0  5.0  20:00:00  79  21:00:00   73  21:00-22:00   \n",
       "3    6:30-7:00  50.0  10.0  5.0   7:30:00  70   8:00:00   78  13:30-14:30   \n",
       "4    3:00-4:00  50.0   9.0  5.0   4:00:00  80   5:00:00   73    5:00-6:00   \n",
       "\n",
       "      B1   B2   B3           B4        B5  B6        B7    B8           B9  \\\n",
       "0  350.0  3.5  3.5    7:00-8:00   8:00:00  65  11:30:00  45.0  11:30-13:00   \n",
       "1  320.0  3.5  3.5  22:00-23:00  23:00:00  80   6:00:00  45.0    6:00-7:30   \n",
       "2  320.0  3.5  3.5  22:00-23:00  23:00:00  80   1:00:00  45.0    1:00-2:30   \n",
       "3  290.0  3.5  3.5  14:30-15:30  15:30:00  65  18:00:00  45.0  19:00-20:30   \n",
       "4  320.0  3.5  3.5    6:00-7:00   7:00:00  80   9:00:00  45.0   9:00-10:30   \n",
       "\n",
       "           B10          B11     B12   B13  B14     收率  \n",
       "0  14:00-15:30          NaN   800.0  0.15  400  0.879  \n",
       "1    7:30-9:00   9:00-10:00  1200.0  0.15  400  0.902  \n",
       "2    2:30-4:00    4:00-5:00  1200.0  0.15  400  0.936  \n",
       "3  21:30-23:00          NaN   800.0  0.15  400  0.902  \n",
       "4  10:30-12:00  12:00-13:00  1200.0  0.15  420  0.983  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df.drop(['B3', 'B13', 'A13', 'A18', 'A23'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cols = list(train.columns)\n",
    "# for col in train.columns:\n",
    "#     rate = train[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "#     if rate > 0.9:\n",
    "#         good_cols.remove(col)\n",
    "#         print(col,rate)\n",
    "\n",
    "train = train[train['收率']>0.865]\n",
    "        \n",
    "train = train[good_cols]\n",
    "good_cols.remove('收率')\n",
    "test  = test[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['收率']\n",
    "del train['收率']\n",
    "data = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "data = data.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeTranSecond(t):\n",
    "    try:\n",
    "        t,m,s=t.split(\":\")\n",
    "    except:\n",
    "        if t=='1900/1/9 7:00':\n",
    "            return 7*3600/3600\n",
    "        elif t=='1900/1/1 2:30':\n",
    "            return (2*3600+30*60)/3600\n",
    "        elif t==-1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    try:\n",
    "        tm = (int(t)*3600+int(m)*60+int(s))/3600\n",
    "    except:\n",
    "        return (30*60)/3600\n",
    "    \n",
    "    return tm\n",
    "for f in ['A5','A7','A9','A11','A14','A16','A24','A26','B5','B7']:\n",
    "    data[f] = data[f].apply(timeTranSecond)\n",
    "\n",
    "def getDuration(se):\n",
    "    try:\n",
    "        sh,sm,eh,em=re.findall(r\"\\d+\\.?\\d*\",se)\n",
    "    except:\n",
    "        if se == -1:\n",
    "            return -1 \n",
    "        \n",
    "    try:\n",
    "        if int(sh)>int(eh):\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600 + 24\n",
    "        else:\n",
    "            tm = (int(eh)*3600+int(em)*60-int(sm)*60-int(sh)*3600)/3600\n",
    "    except:\n",
    "        if se=='19:-20:05':\n",
    "            return 1\n",
    "        elif se=='15:00-1600':\n",
    "            return 1\n",
    "    \n",
    "    return tm\n",
    "\n",
    "def get_start(se):\n",
    "    try:\n",
    "        sh, sm, eh, em = re.findall(r'\\d+\\.?d*', se)\n",
    "    except:\n",
    "        if se == -1:\n",
    "            return -1\n",
    "    try:\n",
    "        tm = (int(eh) * 3600 + int(em) * 60)/3600\n",
    "    except:\n",
    "        if se == '19:-20:05':\n",
    "            return 19\n",
    "        elif se == '15:00-1600':\n",
    "            return 15\n",
    "    return tm\n",
    "for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "    data[f+'_diff'] = data.apply(lambda df: getDuration(df[f]), axis=1)\n",
    "for f in ['A20','A28','B4','B9','B10','B11']:\n",
    "    data[f] = data.apply(lambda df: get_start(df[f]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['样本id'] = data['样本id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "categorical_columns = [f for f in data.columns if f not in ['样本id']]\n",
    "numerical_columns = [f for f in data.columns if f not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1389, 44)\n",
      "(150, 44)\n"
     ]
    }
   ],
   "source": [
    "for f in categorical_columns:\n",
    "    data[f] = data[f].map(dict(zip(data[f].unique(), range(0, data[f].nunique()))))\n",
    "train = data[:train.shape[0]]\n",
    "test  = data[train.shape[0]:]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1389, 149)\n",
      "(150, 149)\n"
     ]
    }
   ],
   "source": [
    "train['target'] = target\n",
    "train['intTarget'] = pd.cut(train['target'], 5, labels=False)\n",
    "train = pd.get_dummies(train, columns=['intTarget'])\n",
    "li = ['intTarget_0.0','intTarget_1.0','intTarget_2.0','intTarget_3.0','intTarget_4.0']\n",
    "mean_columns = []\n",
    "for f1 in categorical_columns:\n",
    "    cate_rate = train[f1].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if cate_rate < 0.90:\n",
    "        for f2 in li:\n",
    "            col_name = 'B14_to_'+f1+\"_\"+f2+'_mean'\n",
    "            mean_columns.append(col_name)\n",
    "            order_label = train.groupby([f1])[f2].mean()\n",
    "            train[col_name] = train['B14'].map(order_label)\n",
    "            miss_rate = train[col_name].isnull().sum() * 100 / train[col_name].shape[0]\n",
    "            if miss_rate > 0:\n",
    "                train = train.drop([col_name], axis=1)\n",
    "                mean_columns.remove(col_name)\n",
    "            else:\n",
    "                test[col_name] = test['B14'].map(order_label)\n",
    "                \n",
    "train.drop(li+['target'], axis=1, inplace=True)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[mean_columns+numerical_columns].values\n",
    "X_test = test[mean_columns+numerical_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1389, 1707)\n",
      "(150, 1707)\n"
     ]
    }
   ],
   "source": [
    "# one hot\n",
    "enc = OneHotEncoder()\n",
    "for f in categorical_columns:\n",
    "    enc.fit(data[f].values.reshape(-1, 1))\n",
    "    X_train = sparse.hstack((X_train, enc.transform(train[f].values.reshape(-1, 1))), 'csr')\n",
    "    X_test = sparse.hstack((X_test, enc.transform(test[f].values.reshape(-1, 1))), 'csr')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_params = {'learning_rate': 0.1,\n",
    "#               'depth': 12,\n",
    "#               'l2_leaf_reg': 10,\n",
    "#               'bootstrap_type': 'Bernoulli',\n",
    "#               'od_type': 'Iter',\n",
    "#               'od_wait': 50,\n",
    "#               'random_seed': 11,\n",
    "#               'allow_writing_files': False}\n",
    "# folds = KFold(n_splits=5, shuffle=True, random_state=666)\n",
    "# oof_cat = np.zeros(len(train))\n",
    "# predictions_cat = np.zeros(len(test))\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "#     print(f\"fold {fold_+1}\")\n",
    "#     (trn_data_X, trn_data_y, val_data_X, val_data_y) = (\n",
    "#         X_train.toarray()[trn_idx],\n",
    "#         y_train[trn_idx],\n",
    "#         X_train.toarray()[val_idx],\n",
    "#         y_train[val_idx])\n",
    "#     clf = CatBoostRegressor(iterations=20000, eval_metric='RMSE', use_best_model=True, **cat_params)\n",
    "#     clf.fit(trn_data_X, trn_data_y,\n",
    "#             eval_set=(val_data_X, val_data_y),\n",
    "#             logging_level='Verbose')\n",
    "#     oof_cat[val_idx] = clf.predict(val_data_X)\n",
    "    \n",
    "#     predictions_cat += clf.predict(X_test.toarray()) / folds.n_splits\n",
    "    \n",
    "# print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_cat, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000232813\tvalid_1's l2: 0.000250289\n",
      "[400]\ttraining's l2: 0.000122191\tvalid_1's l2: 0.000163091\n",
      "[600]\ttraining's l2: 9.07421e-05\tvalid_1's l2: 0.000146635\n",
      "[800]\ttraining's l2: 7.51469e-05\tvalid_1's l2: 0.00014281\n",
      "[1000]\ttraining's l2: 6.55165e-05\tvalid_1's l2: 0.000142481\n",
      "Early stopping, best iteration is:\n",
      "[903]\ttraining's l2: 6.96935e-05\tvalid_1's l2: 0.000142248\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.0002322\tvalid_1's l2: 0.000249208\n",
      "[400]\ttraining's l2: 0.000123578\tvalid_1's l2: 0.000142156\n",
      "[600]\ttraining's l2: 9.22198e-05\tvalid_1's l2: 0.000118555\n",
      "[800]\ttraining's l2: 7.65112e-05\tvalid_1's l2: 0.000111791\n",
      "[1000]\ttraining's l2: 6.63761e-05\tvalid_1's l2: 0.000109989\n",
      "[1200]\ttraining's l2: 5.9367e-05\tvalid_1's l2: 0.000109886\n",
      "Early stopping, best iteration is:\n",
      "[1135]\ttraining's l2: 6.13921e-05\tvalid_1's l2: 0.000109702\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000228584\tvalid_1's l2: 0.000272042\n",
      "[400]\ttraining's l2: 0.000122648\tvalid_1's l2: 0.000170006\n",
      "[600]\ttraining's l2: 9.19911e-05\tvalid_1's l2: 0.000144319\n",
      "[800]\ttraining's l2: 7.56481e-05\tvalid_1's l2: 0.000134328\n",
      "[1000]\ttraining's l2: 6.48853e-05\tvalid_1's l2: 0.000130258\n",
      "[1200]\ttraining's l2: 5.73978e-05\tvalid_1's l2: 0.000129011\n",
      "Early stopping, best iteration is:\n",
      "[1281]\ttraining's l2: 5.49133e-05\tvalid_1's l2: 0.000128641\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000221446\tvalid_1's l2: 0.000256481\n",
      "[400]\ttraining's l2: 0.00011588\tvalid_1's l2: 0.000183876\n",
      "[600]\ttraining's l2: 8.70919e-05\tvalid_1's l2: 0.000166034\n",
      "[800]\ttraining's l2: 7.27863e-05\tvalid_1's l2: 0.000157937\n",
      "[1000]\ttraining's l2: 6.35331e-05\tvalid_1's l2: 0.000154235\n",
      "[1200]\ttraining's l2: 5.70518e-05\tvalid_1's l2: 0.000152918\n",
      "[1400]\ttraining's l2: 5.20213e-05\tvalid_1's l2: 0.000152465\n",
      "[1600]\ttraining's l2: 4.79325e-05\tvalid_1's l2: 0.000152215\n",
      "[1800]\ttraining's l2: 4.46328e-05\tvalid_1's l2: 0.000152065\n",
      "Early stopping, best iteration is:\n",
      "[1869]\ttraining's l2: 4.35566e-05\tvalid_1's l2: 0.000151906\n",
      "fold 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining's l2: 0.000228679\tvalid_1's l2: 0.000265396\n",
      "[400]\ttraining's l2: 0.000119636\tvalid_1's l2: 0.000174015\n",
      "[600]\ttraining's l2: 8.84392e-05\tvalid_1's l2: 0.000153193\n",
      "[800]\ttraining's l2: 7.31384e-05\tvalid_1's l2: 0.000144493\n",
      "[1000]\ttraining's l2: 6.34997e-05\tvalid_1's l2: 0.000140766\n",
      "[1200]\ttraining's l2: 5.6813e-05\tvalid_1's l2: 0.000138989\n",
      "[1400]\ttraining's l2: 5.1812e-05\tvalid_1's l2: 0.000138135\n",
      "[1600]\ttraining's l2: 4.75932e-05\tvalid_1's l2: 0.000137764\n",
      "[1800]\ttraining's l2: 4.41859e-05\tvalid_1's l2: 0.00013755\n",
      "Early stopping, best iteration is:\n",
      "[1800]\ttraining's l2: 4.41859e-05\tvalid_1's l2: 0.00013755\n",
      "CV score: 0.00013401\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 120,\n",
    "         'min_data_in_leaf': 12, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.005,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.8,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.8,\n",
    "         \"bagging_seed\": 6,\n",
    "         \"metric\": 'mse',\n",
    "         \"lambda_l1\": 0.01,\n",
    "         \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=666)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(f\"fold {fold_+1}\")\n",
    "    trn_data = lgb.Dataset(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = lgb.Dataset(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 100)\n",
    "    oof_lgb[val_idx] = clf.predict(X_train[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "[0]\ttrain-rmse:0.422663\tvalid_data-rmse:0.423395\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.256849\tvalid_data-rmse:0.257782\n",
      "[200]\ttrain-rmse:0.156534\tvalid_data-rmse:0.157515\n",
      "[300]\ttrain-rmse:0.095886\tvalid_data-rmse:0.097041\n",
      "[400]\ttrain-rmse:0.059347\tvalid_data-rmse:0.060597\n",
      "[500]\ttrain-rmse:0.037352\tvalid_data-rmse:0.038684\n",
      "[600]\ttrain-rmse:0.024145\tvalid_data-rmse:0.025711\n",
      "[700]\ttrain-rmse:0.016277\tvalid_data-rmse:0.01834\n",
      "[800]\ttrain-rmse:0.011635\tvalid_data-rmse:0.014455\n",
      "[900]\ttrain-rmse:0.008904\tvalid_data-rmse:0.012592\n",
      "[1000]\ttrain-rmse:0.007352\tvalid_data-rmse:0.011761\n",
      "[1100]\ttrain-rmse:0.006403\tvalid_data-rmse:0.011405\n",
      "[1200]\ttrain-rmse:0.005811\tvalid_data-rmse:0.011253\n",
      "[1300]\ttrain-rmse:0.005405\tvalid_data-rmse:0.011203\n",
      "[1400]\ttrain-rmse:0.005057\tvalid_data-rmse:0.011193\n",
      "[1500]\ttrain-rmse:0.004802\tvalid_data-rmse:0.011201\n",
      "Stopping. Best iteration:\n",
      "[1373]\ttrain-rmse:0.00515\tvalid_data-rmse:0.01119\n",
      "\n",
      "fold n°2\n",
      "[0]\ttrain-rmse:0.422562\tvalid_data-rmse:0.423795\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.256797\tvalid_data-rmse:0.257919\n",
      "[200]\ttrain-rmse:0.156491\tvalid_data-rmse:0.157583\n",
      "[300]\ttrain-rmse:0.09583\tvalid_data-rmse:0.096901\n",
      "[400]\ttrain-rmse:0.059277\tvalid_data-rmse:0.06046\n",
      "[500]\ttrain-rmse:0.037239\tvalid_data-rmse:0.038485\n",
      "[600]\ttrain-rmse:0.024038\tvalid_data-rmse:0.025512\n",
      "[700]\ttrain-rmse:0.016151\tvalid_data-rmse:0.018103\n",
      "[800]\ttrain-rmse:0.011511\tvalid_data-rmse:0.014254\n",
      "[900]\ttrain-rmse:0.008794\tvalid_data-rmse:0.012405\n",
      "[1000]\ttrain-rmse:0.007224\tvalid_data-rmse:0.011583\n",
      "[1100]\ttrain-rmse:0.006299\tvalid_data-rmse:0.011246\n",
      "[1200]\ttrain-rmse:0.005681\tvalid_data-rmse:0.011114\n",
      "[1300]\ttrain-rmse:0.005268\tvalid_data-rmse:0.011084\n",
      "[1400]\ttrain-rmse:0.004949\tvalid_data-rmse:0.01108\n",
      "[1500]\ttrain-rmse:0.004648\tvalid_data-rmse:0.011096\n",
      "Stopping. Best iteration:\n",
      "[1346]\ttrain-rmse:0.005117\tvalid_data-rmse:0.011075\n",
      "\n",
      "fold n°3\n",
      "[0]\ttrain-rmse:0.422691\tvalid_data-rmse:0.423267\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.256856\tvalid_data-rmse:0.256656\n",
      "[200]\ttrain-rmse:0.15653\tvalid_data-rmse:0.15587\n",
      "[300]\ttrain-rmse:0.095891\tvalid_data-rmse:0.094853\n",
      "[400]\ttrain-rmse:0.059333\tvalid_data-rmse:0.058128\n",
      "[500]\ttrain-rmse:0.037263\tvalid_data-rmse:0.036346\n",
      "[600]\ttrain-rmse:0.024001\tvalid_data-rmse:0.023663\n",
      "[700]\ttrain-rmse:0.016088\tvalid_data-rmse:0.016643\n",
      "[800]\ttrain-rmse:0.011413\tvalid_data-rmse:0.01315\n",
      "[900]\ttrain-rmse:0.008687\tvalid_data-rmse:0.011602\n",
      "[1000]\ttrain-rmse:0.007112\tvalid_data-rmse:0.011001\n",
      "[1100]\ttrain-rmse:0.006186\tvalid_data-rmse:0.010812\n",
      "[1200]\ttrain-rmse:0.005622\tvalid_data-rmse:0.010766\n",
      "[1300]\ttrain-rmse:0.005224\tvalid_data-rmse:0.010765\n",
      "[1400]\ttrain-rmse:0.004927\tvalid_data-rmse:0.010783\n",
      "Stopping. Best iteration:\n",
      "[1203]\ttrain-rmse:0.005603\tvalid_data-rmse:0.010763\n",
      "\n",
      "fold n°4\n",
      "[0]\ttrain-rmse:0.423132\tvalid_data-rmse:0.421509\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.257155\tvalid_data-rmse:0.255895\n",
      "[200]\ttrain-rmse:0.156701\tvalid_data-rmse:0.155615\n",
      "[300]\ttrain-rmse:0.095952\tvalid_data-rmse:0.095109\n",
      "[400]\ttrain-rmse:0.059336\tvalid_data-rmse:0.058922\n",
      "[500]\ttrain-rmse:0.037261\tvalid_data-rmse:0.037552\n",
      "[600]\ttrain-rmse:0.024002\tvalid_data-rmse:0.025201\n",
      "[700]\ttrain-rmse:0.016089\tvalid_data-rmse:0.018424\n",
      "[800]\ttrain-rmse:0.011427\tvalid_data-rmse:0.015116\n",
      "[900]\ttrain-rmse:0.008665\tvalid_data-rmse:0.013601\n",
      "[1000]\ttrain-rmse:0.007071\tvalid_data-rmse:0.012961\n",
      "[1100]\ttrain-rmse:0.006111\tvalid_data-rmse:0.012705\n",
      "[1200]\ttrain-rmse:0.005527\tvalid_data-rmse:0.012596\n",
      "[1300]\ttrain-rmse:0.005134\tvalid_data-rmse:0.012559\n",
      "[1400]\ttrain-rmse:0.004829\tvalid_data-rmse:0.012542\n",
      "[1500]\ttrain-rmse:0.004564\tvalid_data-rmse:0.012535\n",
      "[1600]\ttrain-rmse:0.00433\tvalid_data-rmse:0.012529\n",
      "[1700]\ttrain-rmse:0.004114\tvalid_data-rmse:0.01253\n",
      "[1800]\ttrain-rmse:0.003905\tvalid_data-rmse:0.012535\n",
      "Stopping. Best iteration:\n",
      "[1612]\ttrain-rmse:0.004301\tvalid_data-rmse:0.012524\n",
      "\n",
      "fold n°5\n",
      "[0]\ttrain-rmse:0.42299\tvalid_data-rmse:0.422063\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.257055\tvalid_data-rmse:0.256427\n",
      "[200]\ttrain-rmse:0.156649\tvalid_data-rmse:0.156332\n",
      "[300]\ttrain-rmse:0.095939\tvalid_data-rmse:0.095791\n",
      "[400]\ttrain-rmse:0.059329\tvalid_data-rmse:0.059473\n",
      "[500]\ttrain-rmse:0.037252\tvalid_data-rmse:0.037941\n",
      "[600]\ttrain-rmse:0.023986\tvalid_data-rmse:0.02549\n",
      "[700]\ttrain-rmse:0.016063\tvalid_data-rmse:0.018643\n",
      "[800]\ttrain-rmse:0.011374\tvalid_data-rmse:0.015213\n",
      "[900]\ttrain-rmse:0.008609\tvalid_data-rmse:0.013678\n",
      "[1000]\ttrain-rmse:0.007009\tvalid_data-rmse:0.013041\n",
      "[1100]\ttrain-rmse:0.006089\tvalid_data-rmse:0.012809\n",
      "[1200]\ttrain-rmse:0.005505\tvalid_data-rmse:0.012744\n",
      "[1300]\ttrain-rmse:0.005098\tvalid_data-rmse:0.012736\n",
      "[1400]\ttrain-rmse:0.004818\tvalid_data-rmse:0.012757\n",
      "Stopping. Best iteration:\n",
      "[1253]\ttrain-rmse:0.005266\tvalid_data-rmse:0.012728\n",
      "\n",
      "CV score: 0.00013650\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {'eta': 0.005, 'max_depth': 12, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 4}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=666)\n",
    "oof_xgb = np.zeros(len(train))\n",
    "predictions_xgb = np.zeros(len(test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    clf = xgb.train(dtrain=trn_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=100, params=xgb_params)\n",
    "    oof_xgb[val_idx] = clf.predict(xgb.DMatrix(X_train[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "    predictions_xgb += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "    \n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.00005554\n"
     ]
    }
   ],
   "source": [
    "oof_h2o = pd.read_csv('oof_h2o.csv', header=None).values.reshape(-1)\n",
    "predictions_h2o = pd.read_csv('predictions_h2o.csv', header=None).values.reshape(-1)\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_h2o, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof_lgb, oof_xgb, oof_h2o]).transpose()\n",
    "test_stack = np.vstack([predictions_lgb, predictions_xgb, predictions_h2o]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982517</td>\n",
       "      <td>0.978979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.982517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.978376</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.982517  0.978979\n",
       "1  0.982517  1.000000  0.978376\n",
       "2  0.978979  0.978376  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_stack).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "[-0.48566608 -0.53455871  1.97526539]\n",
      "fold 1\n",
      "[-0.48128957 -0.52969888  1.96108894]\n",
      "fold 2\n",
      "[-0.46049419 -0.50909938  1.92552766]\n",
      "fold 3\n",
      "[-0.49912347 -0.4673784   1.92846176]\n",
      "fold 4\n",
      "[-0.5758198  -0.55793113  2.09283898]\n",
      "fold 5\n",
      "[-0.47873969 -0.544317    1.97859045]\n",
      "fold 6\n",
      "[-0.48650093 -0.5029391   1.94208937]\n",
      "fold 7\n",
      "[-0.52944712 -0.51479517  2.00356837]\n",
      "fold 8\n",
      "[-0.48748625 -0.56396863  2.0043011 ]\n",
      "fold 9\n",
      "[-0.51149206 -0.46934334  1.94308696]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2025431844908444e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=4590)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "    \n",
    "    clf_3 = LinearRegression()\n",
    "    clf_3.fit(trn_data, trn_y)\n",
    "    print(clf_3.coef_)\n",
    "    \n",
    "    oof_stack[val_idx] = clf_3.predict(val_data)\n",
    "    predictions += clf_3.predict(test_stack) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('jinnan_round1_submit_20181227.csv', header=None)\n",
    "sub_df[1] = predictions\n",
    "sub_df[1] = sub_df[1].apply(lambda x:round(x, 3))\n",
    "sub_df.to_csv(\"submit.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
